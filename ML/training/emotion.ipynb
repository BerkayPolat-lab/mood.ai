{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be74f893",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Emotion Classifier Training with RAVDESS Dataset\n",
    "\n",
    "# This notebook fine-tunes the HuBERT model (`superb/hubert-base-superb-er`) for emotion classification using the RAVDESS dataset.\n",
    "\n",
    "# **Dataset**: RAVDESS (Ryerson Audio-Visual Database of Emotional Speech and Song)\n",
    "# - 8 emotions: neutral, calm, happy, sad, angry, fearful, surprise, disgust\n",
    "# - ~7,350 audio files from 24 actors\n",
    "\n",
    "# **Training Strategy**:\n",
    "# - Freeze all HuBERT layers (keep pre-trained features)\n",
    "# - Train only the classification head (8 classes)\n",
    "# - Speaker-independent train/val/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5362fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from transformers import (\n",
    "    AutoModelForAudioClassification,\n",
    "    AutoFeatureExtractor,\n",
    "    AutoConfig\n",
    ")\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# RAVDESS emotion labels (8 classes)\n",
    "RAVDESS_EMOTIONS = [\n",
    "    \"neutral\",\n",
    "    \"calm\",\n",
    "    \"happy\",\n",
    "    \"sad\",\n",
    "    \"angry\",\n",
    "    \"fearful\",\n",
    "    \"surprise\",\n",
    "    \"disgust\"\n",
    "]\n",
    "\n",
    "EMOTION_TO_IDX = {emotion: idx for idx, emotion in enumerate(RAVDESS_EMOTIONS)}\n",
    "IDX_TO_EMOTION = {idx: emotion for emotion, idx in EMOTION_TO_IDX.items()}\n",
    "\n",
    "print(f\"Emotion classes: {RAVDESS_EMOTIONS}\")\n",
    "print(f\"Number of classes: {len(RAVDESS_EMOTIONS)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e063d2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 1: Load and Preprocess RAVDESS Dataset\n",
    "\n",
    "# RAVDESS files are named with pattern: `[Modality]-[Vocal]-[Emotion]-[Intensity]-[Statement]-[Repetition]-[Actor].wav`\n",
    "\n",
    "# - Modality: 01=full AV, 02=video-only, 03=audio-only\n",
    "# - Emotion: 01=neutral, 02=calm, 03=happy, 04=sad, 05=angry, 06=fearful, 07=surprise, 08=disgust\n",
    "# - Actor: 01-24 (12 male, 12 female)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fdcd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAVDESSDataset(Dataset):\n",
    "    \"\"\"Dataset class for RAVDESS audio files.\"\"\"\n",
    "    \n",
    "    def __init__(self, file_paths: List[str], labels: List[int], feature_extractor, max_length: int = 16000 * 4):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            file_paths: List of audio file paths\n",
    "            labels: List of emotion labels (indices)\n",
    "            feature_extractor: HuggingFace feature extractor\n",
    "            max_length: Maximum audio length in samples (default: 4 seconds at 16kHz)\n",
    "        \"\"\"\n",
    "        self.file_paths = file_paths\n",
    "        self.labels = labels\n",
    "        self.feature_extractor = feature_extractor\n",
    "        self.max_length = max_length\n",
    "        self.sample_rate = 16000  # RAVDESS is 16kHz\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.file_paths)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        file_path = self.file_paths[idx]\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        # Load audio file\n",
    "        try:\n",
    "            audio, sr = librosa.load(file_path, sr=self.sample_rate)\n",
    "            \n",
    "            # Pad or truncate to max_length\n",
    "            if len(audio) > self.max_length:\n",
    "                audio = audio[:self.max_length]\n",
    "            else:\n",
    "                audio = np.pad(audio, (0, self.max_length - len(audio)), mode='constant')\n",
    "            \n",
    "            inputs = self.feature_extractor(audio, sampling_rate=self.sample_rate, return_tensors=\"pt\", padding=True)\n",
    "            \n",
    "            return {\n",
    "                'input_values': inputs['input_values'].squeeze(0),\n",
    "                'labels': torch.tensor(label, dtype=torch.long)\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "            # Return zeros if file can't be loaded\n",
    "            inputs = self.feature_extractor(\n",
    "                np.zeros(self.max_length),\n",
    "                sampling_rate=self.sample_rate,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=True\n",
    "            )\n",
    "            return {\n",
    "                'input_values': inputs['input_values'].squeeze(0),\n",
    "                'labels': torch.tensor(0, dtype=torch.long)\n",
    "            }\n",
    "\n",
    "\n",
    "def parse_ravdess_filename(filename: str) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Parse RAVDESS filename to extract metadata.\n",
    "    \n",
    "    Format: [Modality]-[Vocal]-[Emotion]-[Intensity]-[Statement]-[Repetition]-[Actor].wav\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with parsed values\n",
    "    \"\"\"\n",
    "    basename = Path(filename).stem\n",
    "    parts = basename.split('-')\n",
    "    \n",
    "    if len(parts) != 7:\n",
    "        return None\n",
    "    \n",
    "    return {\n",
    "        'modality': int(parts[0]),\n",
    "        'vocal': int(parts[1]),\n",
    "        'emotion': int(parts[2]),\n",
    "        'intensity': int(parts[3]),\n",
    "        'statement': int(parts[4]),\n",
    "        'repetition': int(parts[5]),\n",
    "        'actor': int(parts[6])\n",
    "    }\n",
    "\n",
    "\n",
    "def load_ravdess_dataset(data_dir: str) -> Tuple[List[str], List[int], List[int]]:\n",
    "    \"\"\"\n",
    "    Load RAVDESS dataset and extract file paths, labels, and actor IDs.\n",
    "    \n",
    "    Args:\n",
    "        data_dir: Directory containing RAVDESS audio files\n",
    "        \n",
    "    Returns:\n",
    "        Tuple of (file_paths, labels, actors)\n",
    "    \"\"\"\n",
    "    data_dir = Path(data_dir)\n",
    "    file_paths = []\n",
    "    labels = []\n",
    "    actors = []\n",
    "    \n",
    "    # Emotion mapping: RAVDESS uses 01-08, we map to 0-7\n",
    "    # Note: RAVDESS emotion 01=neutral, 02=calm, 03=happy, 04=sad, 05=angry, 06=fearful, 07=surprise, 08=disgust\n",
    "    emotion_mapping = {\n",
    "        1: 0,  # neutral\n",
    "        2: 1,  # calm\n",
    "        3: 2,  # happy\n",
    "        4: 3,  # sad\n",
    "        5: 4,  # angry\n",
    "        6: 5,  # fearful\n",
    "        7: 6,  # surprise\n",
    "        8: 7   # disgust\n",
    "    }\n",
    "    \n",
    "    # Find all WAV files\n",
    "    audio_files = list(data_dir.rglob(\"*.wav\"))\n",
    "    \n",
    "    print(f\"Found {len(audio_files)} audio files\")\n",
    "    \n",
    "    for audio_file in audio_files:\n",
    "        parsed = parse_ravdess_filename(audio_file.name)\n",
    "        if parsed is None:\n",
    "            continue\n",
    "        \n",
    "        # Only use audio-only files (modality 03)\n",
    "        if parsed['modality'] != 3:\n",
    "            continue\n",
    "        \n",
    "        emotion_code = parsed['emotion']\n",
    "        if emotion_code in emotion_mapping:\n",
    "            file_paths.append(str(audio_file))\n",
    "            labels.append(emotion_mapping[emotion_code])\n",
    "            actors.append(parsed['actor'])\n",
    "    \n",
    "    print(f\"Loaded {len(file_paths)} audio-only files\")\n",
    "    print(f\"Emotion distribution: {np.bincount(labels)}\")\n",
    "    \n",
    "    return file_paths, labels, actors\n",
    "\n",
    "\n",
    "# Load dataset\n",
    "# Update this path to your RAVDESS dataset location\n",
    "RAVDESS_DATA_DIR = \"path/to/ravdess/audio_speech_actors_01-24\" \n",
    "\n",
    "# Uncomment when dataset is downloaded:\n",
    "# file_paths, labels, actors = load_ravdess_dataset(RAVDESS_DATA_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f7ebdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Step 2: Train/Validation/Test Split (Speaker-Independent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a4b4d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_speaker_independent_split(file_paths: List[str], labels: List[int], actors: List[int], \n",
    "                                     train_ratio: float = 0.7, val_ratio: float = 0.15):\n",
    "    \"\"\"\n",
    "    Create speaker-independent train/val/test split.\n",
    "    \n",
    "    Args:\n",
    "        file_paths: List of file paths\n",
    "        labels: List of labels\n",
    "        actors: List of actor IDs\n",
    "        train_ratio: Proportion for training\n",
    "        val_ratio: Proportion for validation\n",
    "        \n",
    "    Returns:\n",
    "        Train, validation, and test splits\n",
    "    \"\"\"\n",
    "    # Get unique actors\n",
    "    unique_actors = sorted(set(actors))\n",
    "    num_actors = len(unique_actors)\n",
    "    \n",
    "    # Split actors (not files)\n",
    "    num_train_actors = int(num_actors * train_ratio)\n",
    "    num_val_actors = int(num_actors * val_ratio)\n",
    "    \n",
    "    train_actors = set(unique_actors[:num_train_actors])\n",
    "    val_actors = set(unique_actors[num_train_actors:num_train_actors + num_val_actors])\n",
    "    test_actors = set(unique_actors[num_train_actors + num_val_actors:])\n",
    "    \n",
    "    # Split files based on actor membership\n",
    "    train_files, train_labels = [], []\n",
    "    val_files, val_labels = [], []\n",
    "    test_files, test_labels = [], []\n",
    "    \n",
    "    for file_path, label, actor in zip(file_paths, labels, actors):\n",
    "        if actor in train_actors:\n",
    "            train_files.append(file_path)\n",
    "            train_labels.append(label)\n",
    "        elif actor in val_actors:\n",
    "            val_files.append(file_path)\n",
    "            val_labels.append(label)\n",
    "        elif actor in test_actors:\n",
    "            test_files.append(file_path)\n",
    "            test_labels.append(label)\n",
    "    \n",
    "    print(f\"Train: {len(train_files)} files from {len(train_actors)} actors\")\n",
    "    print(f\"Validation: {len(val_files)} files from {len(val_actors)} actors\")\n",
    "    print(f\"Test: {len(test_files)} files from {len(test_actors)} actors\")\n",
    "    \n",
    "    return (train_files, train_labels), (val_files, val_labels), (test_files, test_labels)\n",
    "\n",
    "\n",
    "# Uncomment when dataset is loaded:\n",
    "# train_data, val_data, test_data = create_speaker_independent_split(file_paths, labels, actors)\n",
    "# train_files, train_labels = train_data\n",
    "# val_files, val_labels = val_data\n",
    "# test_files, test_labels = test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1f1257f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load feature extractor\n",
    "model_name = \"superb/hubert-base-superb-er\"\n",
    "num_classes = len(RAVDESS_EMOTIONS)\n",
    "\n",
    "print(f\"Loading model: {model_name}\")\n",
    "print(f\"Number of classes: {num_classes}\")\n",
    "\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(model_name, trust_remote_code=True)\n",
    "\n",
    "# Load model config and update for 8 classes\n",
    "config = AutoConfig.from_pretrained(model_name, trust_remote_code=True)\n",
    "config.num_labels = num_classes\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForAudioClassification.from_pretrained(\n",
    "    model_name,\n",
    "    config=config,\n",
    "    trust_remote_code=True,\n",
    "    ignore_mismatched_sizes=True\n",
    ")\n",
    "\n",
    "# Get hidden size\n",
    "hidden_size = getattr(config, 'hidden_size', 768)\n",
    "\n",
    "# Try to get from existing classifier\n",
    "if hasattr(model, 'classifier'):\n",
    "    if isinstance(model.classifier, nn.Linear):\n",
    "        hidden_size = model.classifier.in_features\n",
    "    elif isinstance(model.classifier, nn.Sequential):\n",
    "        for layer in reversed(model.classifier):\n",
    "            if isinstance(layer, nn.Linear):\n",
    "                hidden_size = layer.in_features\n",
    "                break\n",
    "\n",
    "print(f\"Hidden size: {hidden_size}\")\n",
    "\n",
    "# Replace classification head with new 8-class classifier\n",
    "new_classifier = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "if hasattr(model, 'classification_head'):\n",
    "    model.classification_head = new_classifier\n",
    "else:\n",
    "    model.classifier = new_classifier\n",
    "\n",
    "# Freeze all layers except the classifier\n",
    "for name, param in model.named_parameters():\n",
    "    if 'classifier' in name or 'classification_head' in name:\n",
    "        param.requires_grad = True\n",
    "        print(f\"Training: {name}\")\n",
    "    else:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTrainable parameters: {trainable_params:,} / {total_params:,} ({100 * trainable_params / total_params:.2f}%)\")\n",
    "\n",
    "model = model.to(device)\n",
    "print(f\"Model moved to {device}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880fb2f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create datasets\n",
    "# Uncomment when data is loaded:\n",
    "# train_dataset = RAVDESSDataset(train_files, train_labels, feature_extractor)\n",
    "# val_dataset = RAVDESSDataset(val_files, val_labels, feature_extractor)\n",
    "# test_dataset = RAVDESSDataset(test_files, test_labels, feature_extractor)\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 8  # Adjust based on GPU memory\n",
    "# train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)\n",
    "\n",
    "print(\"Data loaders created (uncomment when dataset is loaded)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc6386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "learning_rate = 1e-4\n",
    "num_epochs = 15\n",
    "patience = 3  # Early stopping patience\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(\n",
    "    filter(lambda p: p.requires_grad, model.parameters()),\n",
    "    lr=learning_rate,\n",
    "    weight_decay=0.01\n",
    ")\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=3, verbose=True\n",
    ")\n",
    "\n",
    "print(\"Training setup complete\")\n",
    "print(f\"Learning rate: {learning_rate}\")\n",
    "print(f\"Epochs: {num_epochs}\")\n",
    "print(f\"Optimizer: AdamW\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789f9dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    \"\"\"Train for one epoch.\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "    for batch in progress_bar:\n",
    "        input_values = batch['input_values'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        outputs = model(input_values=input_values)\n",
    "        logits = outputs.logits\n",
    "        \n",
    "        loss = criterion(logits, labels)\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(logits.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        progress_bar.set_postfix({\n",
    "            'loss': loss.item(),\n",
    "            'acc': 100 * correct / total\n",
    "        })\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy\n",
    "\n",
    "\n",
    "def validate(model, val_loader, criterion, device):\n",
    "    \"\"\"Validate the model.\"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_loader, desc=\"Validating\"):\n",
    "            input_values = batch['input_values'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            outputs = model(input_values=input_values)\n",
    "            logits = outputs.logits\n",
    "            \n",
    "            loss = criterion(logits, labels)\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            _, predicted = torch.max(logits.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    avg_loss = total_loss / len(val_loader)\n",
    "    accuracy = 100 * correct / total\n",
    "    return avg_loss, accuracy, all_preds, all_labels\n",
    "\n",
    "\n",
    "# Training loop\n",
    "# Uncomment when data loaders are ready:\n",
    "\"\"\"\n",
    "best_val_loss = float('inf')\n",
    "best_val_acc = 0\n",
    "patience_counter = 0\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Train\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc, val_preds, val_labels = validate(model, val_loader, criterion, device)\n",
    "    val_losses.append(val_loss)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "    print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # Save best model\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        best_val_acc = val_acc\n",
    "        patience_counter = 0\n",
    "        \n",
    "        # Save model\n",
    "        os.makedirs(\"checkpoints\", exist_ok=True)\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'val_loss': val_loss,\n",
    "            'val_acc': val_acc,\n",
    "        }, \"checkpoints/best_model.pt\")\n",
    "        print(\"âœ“ Saved best model\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Early stopping\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nBest validation accuracy: {best_val_acc:.2f}%\")\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2a190bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test set\n",
    "# Uncomment when training is complete:\n",
    "\"\"\"\n",
    "# Load best model\n",
    "checkpoint = torch.load(\"checkpoints/best_model.pt\")\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "print(\"Loaded best model from checkpoint\")\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_acc, test_preds, test_labels = validate(model, test_loader, criterion, device)\n",
    "print(f\"\\nTest Results:\")\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_acc:.2f}%\")\n",
    "\n",
    "# Classification report\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(\n",
    "    test_labels,\n",
    "    test_preds,\n",
    "    target_names=RAVDESS_EMOTIONS,\n",
    "    digits=4\n",
    "))\n",
    "\n",
    "# Confusion matrix\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "cm = confusion_matrix(test_labels, test_preds)\n",
    "print(cm)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7629ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final model\n",
    "# Uncomment when training is complete:\n",
    "\"\"\"\n",
    "save_dir = \"../model/checkpoints/emotion_classifier_ravdess\"\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# Save model and tokenizer\n",
    "model.save_pretrained(save_dir)\n",
    "feature_extractor.save_pretrained(save_dir)\n",
    "\n",
    "# Also save PyTorch state dict\n",
    "torch.save(model.state_dict(), os.path.join(save_dir, \"pytorch_model.bin\"))\n",
    "\n",
    "print(f\"Model saved to {save_dir}\")\n",
    "print(\"Model is ready to use in worker.py!\")\n",
    "\"\"\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
